{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9/6/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.ver: 2.17.0\n",
      "keras_ver.: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow.ver:\", tf.__version__)\n",
    "print(\"keras_ver.:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver.: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python ver.:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN (Perceptron) with 4 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8\n",
      "6.8\n"
     ]
    }
   ],
   "source": [
    "# simple NN (Perceptron) with 4 inputs\n",
    "inputs = [1, 2, 3, 4.5] # <- this is an array\n",
    "weights = [0.2, 0.8, -0.5, 1]\n",
    "bias = 2\n",
    "\n",
    "# the output is a LC of inputs and weights + bias\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)\n",
    "\n",
    "lc = np.dot(inputs, weights) + bias\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about if there are 4 inputs with 3 neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# 4 inputs and 3 neurons\n",
    "# there are 3 sets per weight\n",
    "inputs = [1, 2, 3, 2.5] # <- this is an array\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "# need 1 bias per neuron, so we need 3 neurons\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "# use list to obtain output\n",
    "# in python, a list is just a matrix\n",
    "output = [\n",
    "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
    "]\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the 3 weights into one matrix; same thing with the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use weights and bias in list format\n",
    "# shape of weights: (3,4) or [3 x 4]: 2D array (matrix)\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2, 0.3, 0.5]\n",
    "# shape of biases: (1,3) or [1 x 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear combination is the **dot product**!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np output: [4.8 3.1 3.3]\n"
     ]
    }
   ],
   "source": [
    "# employ dot product\n",
    "import numpy as np\n",
    "\n",
    "output = np.dot(inputs, weights) + biases\n",
    "print(f\"np output: {output}\") # <- this is called an f-string!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation vs Backpropagation\n",
    "\n",
    "#### Forward Propagation\n",
    "- In forward propagation, the neural network processes input data by passing it through the layers of the network. Each layer applies weights and biases to the inputs, and then an activation function produces the output.\n",
    "- **Purpose**: It calculates the predicted output using the initial weights and biases. No learning or adjustments happen during this phase.\n",
    "- **Process**: Inputs → Weights & Biases → Activation → Output (Prediction)\n",
    "- This phase doesn't involve corrections but simply computes the output based on the current state of the network.\n",
    "\n",
    "#### Backpropagation\n",
    "- Backpropagation is the process where the neural network learns by correcting errors. It calculates the gradient of the loss function with respect to each weight by using the chain rule.\n",
    "- **Purpose**: It updates the weights and biases by minimizing the error between the predicted output and the actual output using techniques like gradient descent.\n",
    "- **Process**: Error (Loss) is propagated back through the network → Calculate gradients → Adjust weights and biases to reduce error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propogation in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1376 0.5652 0.2006 0.037  0.4463 0.4771]\n",
      "[0.3699 0.8701 0.5704]\n",
      "x1 is 0.5 and x2 is 0.85\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "weights = np.around(np.random.uniform(size=6), decimals=4)\n",
    "biases = np.around(np.random.uniform(size=3), decimals=4)\n",
    "\n",
    "print(weights)\n",
    "print(biases)\n",
    "\n",
    "# inputs\n",
    "x_1 = 0.5 # input 1\n",
    "x_2 = 0.85 # input 2\n",
    "\n",
    "print(f\"x1 is {x_1} and x2 is {x_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the first node in the hidden layer is 0.9191\n"
     ]
    }
   ],
   "source": [
    "# z_11 = x1w1 + x2w2 + b11\n",
    "z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]\n",
    "# np.around(z_11, decimals=4)\n",
    "print(f\"The weighted sum of the inputs at the first node in the hidden layer is {z_11:.4f}\")\n",
    "#print(f\"The weighted sum of the inputs at the first node in the hidden layer is {z_11}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the second node in the hidden layer is 1.0018\n"
     ]
    }
   ],
   "source": [
    "# z_12 = x1w3 + x2w4 + b12\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]\n",
    "z_12 = np.around(z_12, decimals=4)\n",
    "print(f\"The weighted sum of the inputs at the second node in the hidden layer is {z_12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Sigmoid activation function: }$ $$a(z) = \\frac{1}{1 + e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation of the first node in the hidden layer is 0.7149\n",
      "The activation of the second node in the hidden layer is 0.7314\n"
     ]
    }
   ],
   "source": [
    "# assuming a sigmoid activation function, let's compute the activation of the first node: a_1,1, in the hidden layer\n",
    "a_11 = 1./(1+np.exp(-z_11))\n",
    "print(f\"The activation of the first node in the hidden layer is {a_11:.4f}\")\n",
    "\n",
    "# for a_1,2\n",
    "a_12 = 1./(1+np.exp(-z_12))\n",
    "print(f\"The activation of the second node in the hidden layer is {a_12:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum at the output layer is 1.2384\n"
     ]
    }
   ],
   "source": [
    "# output layer\n",
    "z_2 = a_11*weights[4] + a_12*weights[5] + biases[2]\n",
    "print(f\"The weighted sum at the output layer is {z_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the network for X is 0.7753\n"
     ]
    }
   ],
   "source": [
    "# output of the network in the output layer\n",
    "a_2 = 1./(1.+np.exp(-z_2))\n",
    "print(f\"The output of the network for X is {a_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# batches of inputs\n",
    "# list of shape (3,4) input matrix\n",
    "inputs = [\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2],\n",
    "    [-1.5, 2.7, 3.3, -.8]\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1885  -1.04185 -2.03875]\n",
      " [ 0.714   -2.7332  -5.7633 ]\n",
      " [-1.2664   1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "# so far, we have input layer with 4 features and a hidden layer with 3 neurons\n",
    "# add another layer with 3 neurons\n",
    "weights2 = [\n",
    "    [0.1, -0.4, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]\n",
    "]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer2_output = np.dot(layer_output, np.array(weights2).T) + biases2\n",
    "print(layer2_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
