{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT479 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a binary classifier using the Breast Cancer Wisconsin (Diagnostic) Dataset from the UCI Machine Learning Repository, a widely used dataset for binary classification tasks. In this assignment, you will build, train, and evaluate a feedforward neural network (FNN) for predicting whether a tumor is malignant or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Use the load_breast_cancer() function from sklearn.datasets to load the breast cancer dataset.\n",
    "\n",
    "• What is the shape of the data (number of samples and features)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the breast cancer data: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# loading the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "data_shape = data.data.shape\n",
    "\n",
    "# displaying shape of data [samples x features]\n",
    "print(\"Shape of the breast cancer data:\", data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Convert the dataset into a pandas DataFrame using the pd.DataFrame() function. Make sure to specify columns= to display the feature names.\n",
    "\n",
    "• What is the mean concavity for observation #564?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# converting into pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3174"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the mean concavity for observation #564\n",
    "obs_564 = df.loc[563, 'mean concavity'] # 0-based index\n",
    "obs_564\n",
    "\n",
    "# The mean concavity for observation #564 is 0.3174."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Split the data into three sets: training, validation, and test sets. Use an 80-10-10 split. Use train_test_split() with the test_size parameter to first split the data into 80% training and 20% temporary data. Then split the temporary data again into 50% validation and 50% test data.\n",
    "• What are the shapes of three datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (57, 30) (57, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df.drop(columns='target'), df['target']\n",
    "\n",
    "# split into 80% training and 20% temporary data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# split the 20% temporary data into 50% validation and 50% test data --> 10 validation and 10 test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Output the shapes of the datasets\n",
    "train_shape = X_train.shape\n",
    "val_shape = X_val.shape\n",
    "test_shape = X_test.shape\n",
    "\n",
    "print(train_shape, val_shape, test_shape)\n",
    "# training set: 455 x 30\n",
    "# validation set: 57 x 30\n",
    "# test set: 57 x 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Standardize the training, validation, and test datasets using StandardScaler() from sklearn.preprocessing. Fit the scaler on the training data and transform the training dataset using .fit_transform(). Apply .transform() to both validation and test dataset using the same scaler fitted on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# fitting scaler on the training data and transforming the training dataset\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# applying .transform() to both validation and test dataset\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Create a feedforward neural network (FNN) using tensorflow.keras (or keras):\n",
    "The network should include one hidden layer with 16 neurons and ReLU activation and an output layer with 1 neuron and Sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# build the model using tensorflow\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(30,)), # input layer with 30 features\n",
    "    tf.keras.layers.Dense(16, activation='relu'), # first hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # output layer with 1 node for classification. Sigmoid activation for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Compile the model using binary_crossentropy loss function, Adam optimizer. Track accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer = 'adam', # adaptive moment estimation\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Train the model for 100 epochs, but include the validation data during training by passing the validation_data argument to model.fit(). Use the training data for training and validation data for monitoring overfitting. Record the loss and accuracy for both training and validation data at each epoch.\n",
    "• What is the validation accuracy after 100 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5592 - loss: 0.7031 - val_accuracy: 0.6842 - val_loss: 0.5235\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7600 - loss: 0.5430 - val_accuracy: 0.7544 - val_loss: 0.4047\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.4548 - val_accuracy: 0.8947 - val_loss: 0.3286\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8847 - loss: 0.3655 - val_accuracy: 0.9123 - val_loss: 0.2784\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.3007 - val_accuracy: 0.9474 - val_loss: 0.2405\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9268 - loss: 0.2483 - val_accuracy: 0.9474 - val_loss: 0.2126\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9366 - loss: 0.2102 - val_accuracy: 0.9474 - val_loss: 0.1911\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9168 - loss: 0.2203 - val_accuracy: 0.9474 - val_loss: 0.1752\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1661 - val_accuracy: 0.9474 - val_loss: 0.1614\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9416 - loss: 0.1741 - val_accuracy: 0.9474 - val_loss: 0.1505\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9610 - loss: 0.1573 - val_accuracy: 0.9474 - val_loss: 0.1417\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.1294 - val_accuracy: 0.9474 - val_loss: 0.1343\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9553 - loss: 0.1504 - val_accuracy: 0.9474 - val_loss: 0.1280\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.1244 - val_accuracy: 0.9474 - val_loss: 0.1233\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.1164 - val_accuracy: 0.9474 - val_loss: 0.1186\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.1234 - val_accuracy: 0.9474 - val_loss: 0.1153\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.1037 - val_accuracy: 0.9474 - val_loss: 0.1116\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.1024 - val_accuracy: 0.9474 - val_loss: 0.1087\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9771 - loss: 0.0989 - val_accuracy: 0.9474 - val_loss: 0.1059\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0942 - val_accuracy: 0.9474 - val_loss: 0.1034\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9813 - loss: 0.0794 - val_accuracy: 0.9474 - val_loss: 0.1021\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0894 - val_accuracy: 0.9474 - val_loss: 0.1004\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0934 - val_accuracy: 0.9474 - val_loss: 0.0987\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0736 - val_accuracy: 0.9474 - val_loss: 0.0974\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0799 - val_accuracy: 0.9474 - val_loss: 0.0965\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0898 - val_accuracy: 0.9474 - val_loss: 0.0955\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0838 - val_accuracy: 0.9474 - val_loss: 0.0947\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0759 - val_accuracy: 0.9474 - val_loss: 0.0939\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0787 - val_accuracy: 0.9474 - val_loss: 0.0934\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.0672 - val_accuracy: 0.9474 - val_loss: 0.0932\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0813 - val_accuracy: 0.9474 - val_loss: 0.0926\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9805 - loss: 0.0738 - val_accuracy: 0.9474 - val_loss: 0.0920\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9790 - loss: 0.0732 - val_accuracy: 0.9474 - val_loss: 0.0910\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9871 - loss: 0.0667 - val_accuracy: 0.9474 - val_loss: 0.0909\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9811 - loss: 0.0741 - val_accuracy: 0.9474 - val_loss: 0.0908\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9803 - loss: 0.0751 - val_accuracy: 0.9649 - val_loss: 0.0900\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.0679 - val_accuracy: 0.9649 - val_loss: 0.0896\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9847 - loss: 0.0692 - val_accuracy: 0.9649 - val_loss: 0.0895\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0599 - val_accuracy: 0.9649 - val_loss: 0.0895\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0527 - val_accuracy: 0.9649 - val_loss: 0.0889\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9841 - loss: 0.0640 - val_accuracy: 0.9649 - val_loss: 0.0889\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0538 - val_accuracy: 0.9649 - val_loss: 0.0884\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0494 - val_accuracy: 0.9649 - val_loss: 0.0878\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9901 - loss: 0.0484 - val_accuracy: 0.9649 - val_loss: 0.0875\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9864 - loss: 0.0535 - val_accuracy: 0.9649 - val_loss: 0.0872\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0535 - val_accuracy: 0.9649 - val_loss: 0.0867\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0436 - val_accuracy: 0.9649 - val_loss: 0.0864\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9940 - loss: 0.0327 - val_accuracy: 0.9649 - val_loss: 0.0861\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0549 - val_accuracy: 0.9825 - val_loss: 0.0868\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0587 - val_accuracy: 0.9649 - val_loss: 0.0866\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9806 - loss: 0.0735 - val_accuracy: 0.9825 - val_loss: 0.0862\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0455 - val_accuracy: 0.9825 - val_loss: 0.0856\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 0.0597 - val_accuracy: 0.9649 - val_loss: 0.0855\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0406 - val_accuracy: 0.9825 - val_loss: 0.0852\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0411 - val_accuracy: 0.9649 - val_loss: 0.0849\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0343 - val_accuracy: 0.9825 - val_loss: 0.0845\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.0460 - val_accuracy: 0.9825 - val_loss: 0.0846\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0486 - val_accuracy: 0.9825 - val_loss: 0.0842\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0336 - val_accuracy: 0.9825 - val_loss: 0.0841\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9734 - loss: 0.0678 - val_accuracy: 0.9649 - val_loss: 0.0833\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0499 - val_accuracy: 0.9649 - val_loss: 0.0831\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0441 - val_accuracy: 0.9649 - val_loss: 0.0827\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9884 - loss: 0.0519 - val_accuracy: 0.9649 - val_loss: 0.0827\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.0582 - val_accuracy: 0.9649 - val_loss: 0.0827\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0382 - val_accuracy: 0.9649 - val_loss: 0.0823\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0363 - val_accuracy: 0.9649 - val_loss: 0.0827\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0327 - val_accuracy: 0.9649 - val_loss: 0.0813\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0374 - val_accuracy: 0.9649 - val_loss: 0.0815\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9909 - loss: 0.0508 - val_accuracy: 0.9649 - val_loss: 0.0815\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9924 - loss: 0.0411 - val_accuracy: 0.9649 - val_loss: 0.0819\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9976 - loss: 0.0254 - val_accuracy: 0.9649 - val_loss: 0.0821\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9945 - loss: 0.0333 - val_accuracy: 0.9649 - val_loss: 0.0823\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0317 - val_accuracy: 0.9649 - val_loss: 0.0819\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0322 - val_accuracy: 0.9649 - val_loss: 0.0817\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9883 - loss: 0.0504 - val_accuracy: 0.9649 - val_loss: 0.0818\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0283 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0413 - val_accuracy: 0.9649 - val_loss: 0.0818\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9952 - loss: 0.0327 - val_accuracy: 0.9649 - val_loss: 0.0818\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9937 - loss: 0.0327 - val_accuracy: 0.9649 - val_loss: 0.0822\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9939 - loss: 0.0304 - val_accuracy: 0.9649 - val_loss: 0.0822\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0399 - val_accuracy: 0.9649 - val_loss: 0.0821\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9917 - loss: 0.0327 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9981 - loss: 0.0244 - val_accuracy: 0.9649 - val_loss: 0.0819\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0351 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9917 - loss: 0.0337 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0236 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9919 - loss: 0.0352 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0363 - val_accuracy: 0.9649 - val_loss: 0.0816\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9907 - loss: 0.0343 - val_accuracy: 0.9649 - val_loss: 0.0817\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0435 - val_accuracy: 0.9649 - val_loss: 0.0814\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9971 - loss: 0.0224 - val_accuracy: 0.9649 - val_loss: 0.0811\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9894 - loss: 0.0436 - val_accuracy: 0.9649 - val_loss: 0.0809\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0271 - val_accuracy: 0.9649 - val_loss: 0.0810\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.0293 - val_accuracy: 0.9649 - val_loss: 0.0814\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9945 - loss: 0.0274 - val_accuracy: 0.9649 - val_loss: 0.0822\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9954 - loss: 0.0288 - val_accuracy: 0.9649 - val_loss: 0.0820\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9960 - loss: 0.0231 - val_accuracy: 0.9649 - val_loss: 0.0814\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9936 - loss: 0.0261 - val_accuracy: 0.9649 - val_loss: 0.0817\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9866 - loss: 0.0388 - val_accuracy: 0.9649 - val_loss: 0.0819\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0297 - val_accuracy: 0.9649 - val_loss: 0.0819\n"
     ]
    }
   ],
   "source": [
    "# training the model for 100 epochs\n",
    "r = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. After training, plot the training loss and validation loss over the 100 epochs. Use this plot to visually determine the point at which the model begins to overfit (i.e., when validation loss stops decreasing and starts to increase).\n",
    "• Based on this plot, determine the optimal number of epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of training loss and validation loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = r.history\n",
    "history_dict.keys()\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss') # 'bo' means blue dot\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss') # 'b' means solid blue line\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Setting the x-ticks to integers and adding breaks at every 5 epochs\n",
    "plt.xticks(range(0, len(loss_values) + 1, 5))  # Adjust the step size to 5\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs seem to be an optimal number of epochs for training, training beyond this point does not seem to improve validation loss and may even lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. According to (h), rebuild your FNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9947 - loss: 0.0240 - val_accuracy: 0.9649 - val_loss: 0.0819\n",
      "Epoch 2/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9907 - loss: 0.0285 - val_accuracy: 0.9649 - val_loss: 0.0818\n",
      "Epoch 3/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9939 - loss: 0.0229 - val_accuracy: 0.9649 - val_loss: 0.0825\n",
      "Epoch 4/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9954 - loss: 0.0251 - val_accuracy: 0.9649 - val_loss: 0.0828\n",
      "Epoch 5/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9908 - loss: 0.0300 - val_accuracy: 0.9649 - val_loss: 0.0833\n",
      "Epoch 6/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9950 - loss: 0.0221 - val_accuracy: 0.9649 - val_loss: 0.0832\n",
      "Epoch 7/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9889 - loss: 0.0322 - val_accuracy: 0.9649 - val_loss: 0.0828\n",
      "Epoch 8/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0246 - val_accuracy: 0.9649 - val_loss: 0.0833\n",
      "Epoch 9/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0283 - val_accuracy: 0.9649 - val_loss: 0.0835\n",
      "Epoch 10/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9926 - loss: 0.0242 - val_accuracy: 0.9649 - val_loss: 0.0830\n",
      "Epoch 11/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0244 - val_accuracy: 0.9649 - val_loss: 0.0834\n",
      "Epoch 12/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9960 - loss: 0.0175 - val_accuracy: 0.9649 - val_loss: 0.0833\n",
      "Epoch 13/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.0174 - val_accuracy: 0.9649 - val_loss: 0.0832\n",
      "Epoch 14/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0252 - val_accuracy: 0.9649 - val_loss: 0.0835\n",
      "Epoch 15/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.0233 - val_accuracy: 0.9649 - val_loss: 0.0835\n",
      "Epoch 16/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9945 - loss: 0.0192 - val_accuracy: 0.9649 - val_loss: 0.0840\n",
      "Epoch 17/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0315 - val_accuracy: 0.9649 - val_loss: 0.0839\n",
      "Epoch 18/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0190 - val_accuracy: 0.9649 - val_loss: 0.0837\n",
      "Epoch 19/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9892 - loss: 0.0273 - val_accuracy: 0.9649 - val_loss: 0.0847\n",
      "Epoch 20/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9929 - loss: 0.0238 - val_accuracy: 0.9649 - val_loss: 0.0850\n",
      "Epoch 21/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9954 - loss: 0.0196 - val_accuracy: 0.9649 - val_loss: 0.0849\n",
      "Epoch 22/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9957 - loss: 0.0184 - val_accuracy: 0.9649 - val_loss: 0.0852\n",
      "Epoch 23/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0270 - val_accuracy: 0.9649 - val_loss: 0.0848\n",
      "Epoch 24/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9950 - loss: 0.0182 - val_accuracy: 0.9649 - val_loss: 0.0850\n",
      "Epoch 25/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9940 - loss: 0.0211 - val_accuracy: 0.9649 - val_loss: 0.0854\n",
      "Epoch 26/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9936 - loss: 0.0200 - val_accuracy: 0.9649 - val_loss: 0.0852\n",
      "Epoch 27/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0153 - val_accuracy: 0.9474 - val_loss: 0.0868\n",
      "Epoch 28/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9974 - loss: 0.0159 - val_accuracy: 0.9474 - val_loss: 0.0868\n",
      "Epoch 29/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9927 - loss: 0.0198 - val_accuracy: 0.9649 - val_loss: 0.0862\n",
      "Epoch 30/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9960 - loss: 0.0166 - val_accuracy: 0.9649 - val_loss: 0.0859\n"
     ]
    }
   ],
   "source": [
    "# rebuilding the FNN model for 30 epochs\n",
    "r2 = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9779 - loss: 0.0488\n",
      "Test accuracy: 0.98246\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high test accuracy suggests that the model generalizes well to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
